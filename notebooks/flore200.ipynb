{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4495258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/projects_lujun/mt_reasoning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 220/220 [00:04<00:00, 47.90files/s]\n",
      "Downloading data: 100%|██████████| 214/214 [00:04<00:00, 53.00files/s]\n",
      "Generating dev split: 100%|██████████| 219340/219340 [00:01<00:00, 121536.90 examples/s]\n",
      "Generating devtest split: 100%|██████████| 216568/216568 [00:01<00:00, 122531.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load all languages with dev and devtest splits\n",
    "ds_full = load_dataset(\"openlanguagedata/flores_plus\")\n",
    "\n",
    "# Load only the dev split for all languages\n",
    "ds_dev = load_dataset(\"openlanguagedata/flores_plus\", split=\"dev\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301e939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_data = []\n",
    "for split in ds_full.keys():\n",
    "    all_data.extend(ds_full[split])\n",
    "\n",
    "df_full = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f57a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_json(\"data/eval/flore200.jsonl\",lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run vllm serve /home/snt/projects_lujun/base_models/Qwen3-4B-Thinking-2507 --gpu_memory_utilization 0.5 --max_model_len 32768 --port 8000 --host 0.0.0.0\n",
    "\n",
    "# uv run vllm serve /home/snt/projects_lujun/base_models/DeepSeek-R1-Distill-Llama-8B --enable-reasoning --reasoning-parser deepseek_r1 --gpu_memory_utilization 0.5 --max_model_len 32768 --port 8000 --host 0.0.0.0 \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from mt_reason.util import prompts_util, clients_util\n",
    "\n",
    "importlib.reload(prompts_util)\n",
    "importlib.reload(clients_util)\n",
    "\n",
    "import os\n",
    "ts = datetime.now().strftime(\"%m%d%H%M\")  \n",
    "\n",
    "#### Setttings\n",
    "\n",
    "df = pd.read_json(\"data/source/flore200.jsonl\", lines=True)\n",
    "openai_api_key = \"sk-pro\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "openai_api_base = \"https://api.openai.com/v1/chat/completions\"\n",
    "# model_path = \"/home/snt/projects_lujun/base_models/Qwen3-4B-Thinking-2507\"\n",
    "model_path = \"/home/snt/projects_lujun/base_models/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_path = \"o3-mini-2025-01-31\"\n",
    "output_path_folder = \"data/outputs/flore200_eval\"\n",
    "iso639_df = prompts_util.load_iso639_df()\n",
    "tgt_langs = [ \"Assamese\", \"Luxembourgish\", \"Maltese\", \"Javanese\", \"Lingala\", \"Hindi\", \"German\", \"Modern Standard Arabic\", \"Standard Malay\",\"Swahili\"]\n",
    "tgt_codes = [ \"asm\", \"ltz\", \"mlt\", \"jav\", \"lin\", \"hin\", \"ger\", \"ara\", \"may\", \"swa\"]\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key, base_url=openai_api_base)\n",
    "eng_code = \"eng\"\n",
    "all_langs_code = df['iso_639_3'].dropna().unique()\n",
    "df_eng = df[df['iso_639_3'] == eng_code]\n",
    "\n",
    "for tgt_code in tgt_codes:\n",
    "    ### English To LRLs\n",
    "    out_path = f\"{output_path_folder}/results_{eng_code}-{tgt_code}_{ts}.jsonl\"\n",
    "    if os.path.exists(out_path):\n",
    "        try:\n",
    "            df_prev = pd.read_json(out_path, lines=True)\n",
    "            idx_prev = len(df_prev)\n",
    "        except Exception:\n",
    "            idx_prev = 0\n",
    "    else:\n",
    "        idx_prev = 0\n",
    "\n",
    "    for idx, row in df_eng.iterrows():\n",
    "        if idx < idx_prev:\n",
    "            continue\n",
    "\n",
    "        update_row = row.copy()\n",
    "        translation_prompt = prompts_util.translate_prompt(row['text'], eng_code, tgt_code)\n",
    "        reasoning_started_at = datetime.now()\n",
    "        translation, translation_reasoning_path = clients_util.generate_with_calling_vllm(openai_api_base, model_path, translation_prompt, api_key=openai_api_key)\n",
    "        reasoning_ended_at = datetime.now()\n",
    "        reasoning_elapsed_sec = (reasoning_ended_at - reasoning_started_at).total_seconds()\n",
    "        update_row['translation'] = translation\n",
    "        update_row['translation_reasoning_path'] = str(translation_reasoning_path)\n",
    "        update_row['reasoning_started_at'] = reasoning_started_at.isoformat()\n",
    "        update_row['reasoning_ended_at'] = reasoning_ended_at.isoformat()\n",
    "        update_row['reasoning_elapsed_sec'] = reasoning_elapsed_sec\n",
    "        updated_df = pd.DataFrame([update_row])\n",
    "        mode = \"w\" if idx_prev == 0 else \"a\"\n",
    "        updated_df.to_json(out_path, orient=\"records\", lines=True, mode=mode)\n",
    "\n",
    "    ### LRLs To English\n",
    "    out_path = f\"{output_path_folder}/results_{tgt_code}-{eng_code}.jsonl\"\n",
    "    if os.path.exists(out_path):\n",
    "        try:\n",
    "            df_prev = pd.read_json(out_path, lines=True)\n",
    "            idx_prev = len(df_prev)\n",
    "        except Exception:\n",
    "            idx_prev = 0\n",
    "    else:\n",
    "        idx_prev = 0\n",
    "\n",
    "    for idx, row in df_eng.iterrows():\n",
    "        if idx < idx_prev:\n",
    "            continue\n",
    "\n",
    "        update_row = row.copy()\n",
    "        translation_prompt = prompts_util.translate_prompt(row['text'], tgt_code, eng_code)\n",
    "        translation = clients_util.generate_with_calling_vllm(openai_api_base, model_path, translation_prompt)\n",
    "        update_row['translation'] = translation\n",
    "        updated_df = pd.DataFrame([update_row])\n",
    "        mode = \"w\" if idx_prev == 0 else \"a\"\n",
    "        updated_df.to_json(out_path, orient=\"records\", lines=True, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv run vllm serve /home/snt/projects_lujun/base_models/Qwen3-4B-Thinking-2507 --gpu_memory_utilization 0.5 --max_model_len 32768 --port 8000 --host 0.0.0.0\n",
    "\n",
    "uv run vllm serve /home/snt/projects_lujun/base_models/DeepSeek-R1-Distill-Llama-8B --enable-reasoning --reasoning-parser deepseek_r1 --gpu_memory_utilization 0.5 --max_model_len 32768 --port 8000 --host 0.0.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8b94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/projects_lujun/mt_reasoning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: Okay, the user asked for a short introduction to large language models. Hmm, they probably want something concise but clear—maybe they're a beginner or just need a quick refresher. I should avoid jargon overload since \"short\" is key here.  \n",
      "\n",
      "First, what's the absolute core? LLMs are AI systems that predict human-like text by learning from massive data. Gotta emphasize \"large\" (billions of parameters!) and \"language\" (text generation, understanding). User might confuse them with chatbots, so I should clarify they're not just for chatting—they do reasoning, coding, etc.  \n",
      "\n",
      "Wait, they said \"short,\" so I'll skip technical details like transformer architecture. But should I mention training data? Yes, briefly—it's why they know facts but have biases. Also, real-world use cases (writing, coding) make it relatable.  \n",
      "\n",
      "*Potential pitfalls*:  \n",
      "- Don't dive into fine-tuning or prompt engineering (too long).  \n",
      "- Avoid calling them \"robots\" (misleading).  \n",
      "- Clarify they're statistical models, not conscious.  \n",
      "\n",
      "User's tone is neutral—likely curious, not technical. I'll end with why they matter (useful tools) to give practical context. Keep it under 5 sentences? *Checks draft*: Yep, 4 sentences cover definition, scale, function, and purpose. Added \"trained on vast text\" to hint at data without geeking out.  \n",
      "\n",
      "*Double-check*: Did I skip anything critical? Oh—bias! Must mention it's a limitation. Added \"can reflect biases\" in the second sentence. Good.  \n",
      "\n",
      "Final thought: User might actually want to know \"how do I use this?\" but they didn't ask, so I'll stick to intro. If they follow up, I can dive deeper.\n",
      "</think>\n",
      "content: Here's a concise introduction to **Large Language Models (LLMs)**:\n",
      "\n",
      "> **Large Language Models (LLMs)** are powerful AI systems trained on vast amounts of text data (like books, websites, and code) to understand and generate human-like language. They predict the next word in a sentence, answer questions, write stories, code, or perform reasoning tasks by learning patterns from their training data. While they excel at many language-related tasks, they are **statistical models**—not conscious entities—and can reflect biases present in their training data.\n",
      "\n",
      "**Why they matter**: LLMs (like GPT-4, Claude, or Llama) power modern chatbots, content creation tools, and applications that make AI interactions feel natural and useful.  \n",
      "\n",
      "*(Total: ~70 words — perfect for a quick overview!)*\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/home/snt/projects_lujun/base_models/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a7dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
